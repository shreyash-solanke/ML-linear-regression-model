# hw4.py

import os.path
import numpy
import math
import warnings
warnings.filterwarnings("ignore")

######################################
#
# FUNCTIONS YOU WILL NEED TO MODIFY:
#  - linreg_closed_form
#  - loss
#  - linreg_grad_desc
#  - random_fourier_features
#
######################################

def linreg_model_sample(Theta,model_X):
	if model_X.shape[1]==1:
		## get a bunch of evenly spaced X values in the same range as the passed in data
		sampled_X = numpy.linspace(model_X.min(axis=0),model_X.max(axis=0),100)
		## get the Y values for our sampled X values by taking the dot-product with the model
		## Note: we're appending a column of all ones so we can do this with a single matrix-vector multiply
		sampled_Y = numpy.hstack([numpy.ones((sampled_X.shape[0],1)),sampled_X]).dot(Theta)
		return sampled_X, sampled_Y
	elif model_X.shape[1]==2:
		## Unfortunately, plotting surfaces is a bit more complicated, first we need
		## a set of points that covers the area we want to plot. numpy.meshgrid is a helper function
		## that will create two NxN arrays that vary over both the X and Y range given.
		sampled_X, sampled_Y = numpy.meshgrid(model_X[:,0],model_X[:,1])
		## We can't just do a simple matrix multiply here, because plot_surface(...) is going to expect NxN arrays like
		## those generated by numpy.meshgrid(...). So here we're explicitly pulling out the components of Theta as
		## scalars and multiplying them across each element in the X and Y arrays to get the value for Z
		sampled_Z = sampled_X*Theta[1]+sampled_Y*Theta[2]+Theta[0]
		return sampled_X, sampled_Y, sampled_Z

def plot_helper(data_X, data_Y, model_X=None, model_Y=None, model_Z=None):
	import matplotlib.pyplot
	## 2D plotting
	## data_X.shape[1] is the number of columns in data_X, just as data_X.shape[0] is the number of rows
	if data_X.shape[1]==1: 
		fig1 = matplotlib.pyplot.figure() ## creates a new figure object that we can plot into
		fig1.gca().scatter(data_X,data_Y) ## creates a scatterplot with the given set of X and Y points
		## If we were given a model, we need to plot that
		if not(model_X is None) and not(model_Y is None):
			## Plot the data from the model
			## Note: we're using plot(...) instead of scatter(...) because we want a smooth curve
			fig1.gca().plot(model_X,model_Y,color='r')
		## The graph won't actually be displayed until we .show(...) it. You can swap this with savefig(...) if you
		## instead want to save an image of the graph instead of displaying it. You can also use the interface to save an
		## image after displaying it
		matplotlib.pyplot.show() #fig1.show()
	## 3D plotting
	elif data_X.shape[1]==2:
		## This import statement 'registers' the ability to do 3D projections/plotting with matplotlib
		from mpl_toolkits.mplot3d import Axes3D
		fig1 = matplotlib.pyplot.figure()
		## The format for 3D scatter is similar to 2D; just add the third dimension to the argument list
		fig1.gca(projection='3d').scatter(data_X[:,0],data_X[:,1],data_Y)
		if not(model_X is None) and not(model_Y is None) and not(model_Z is None):
			## Now, with our X, Y, and Z arrays (all NxN), we can use plot_surface(...) to create a nice 3D surface
			fig1.gca(projection='3d').plot_surface(model_X, model_Y, model_Z,linewidth=0.0,color=(1.0,0.2,0.2,0.75))
		matplotlib.pyplot.show() #fig1.show()
	else:
		## Matplotlib does not yet have the capability to plot in 4D
		print('Data is not in 2 or 3 dimensions, cowardly refusing to plot! (data_X.shape == {})'.format(data_X.shape))

## Data loading utility function
def load_data(fname,directory='data'):
	data = numpy.loadtxt(os.path.join(directory,fname),delimiter=',')
	rows,cols = data.shape
	X_dim = cols-1
	Y_dim = 1
	return data[:,:-1].reshape(-1,X_dim), data[:,-1].reshape(-1,Y_dim)

def vis_linreg_model(train_X, train_Y, Theta):
	sample_X, sample_Y = linreg_model_sample(Theta,train_X)
	#NOTE: this won't work directly with 3D data. Write your own function, or modify this one
	#to generate plots for 2D-noisy-lin.txt or other 3D data.
	plot_helper(train_X, train_Y, sample_X, sample_Y)

def vis_linreg_model_2d(train_X, train_Y, Theta):
	print(train_X.shape, train_Y.shape, Theta.shape)
	sample_X, sample_Y, sample_Z = linreg_model_sample(Theta,train_X)
	#NOTE: this won't work directly with 3D data. Write your own function, or modify this one
	#to generate plots for 2D-noisy-lin.txt or other 3D data.
	plot_helper(train_X, train_Y, sample_X, sample_Y, sample_Z)

###################
# YOUR CODE BELOW #
###################
def linreg_closed_form(train_X, train_Y):
	'''
	Computes the optimal parameters for the given training data in closed form


	Args:
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector
	num

	Returns:
		A length D+1 numpy array with the optimal parameters	
	'''

	n_var = train_X.shape[0]
	arr_identity = numpy.ones((n_var, 1))
	# train_X = numpy.append(arr_identity, train_X, axis=1)
	train_X = numpy.hstack((arr_identity, train_X))

	# train_X = numpy.vstack([train_Y, arr_identity])

	x = train_X
	xt = numpy.transpose(x)
	xtx = numpy.dot(xt, x)

	# print(xtx)
	inv = numpy.linalg.inv(xtx)

	invx = numpy.dot(inv, xt)
	# print("invx: ", invx)
	# print("invx shape: ", invx.shape)
	# print("train_Y shape: ", train_Y.shape)

	Theta = numpy.dot(invx, train_Y)
	# print("x: ", train_X)
	# print("y: ", train_Y)
	#
	# print("xtx: ", xtx)
	# print("inv: ", inv)
	# print("theta: ", Theta)
	return Theta

###################
# YOUR CODE BELOW #
###################
def loss(Theta, train_X, train_Y):
	'''
	Computes the squared loss for the given setting of the parameters given the training data


	Args:
		Theta (length D+1 numpy array): the parameters of the model
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector


	Returns:
		The (scalar) loss for the given parameters and data.
	'''

	# We will compute MSE here

	# print("x ", train_X)
	# print("xshape", train_X.shape)
	n_data_points = train_X.shape[0]
	sum = 0

	n_var = train_X.shape[0]
	arr_identity = numpy.ones([n_var, 1])

	train_X = numpy.append(arr_identity, train_X, axis=1)

	ypredict = numpy.dot(train_X, Theta)

	for each_datapt in range(n_data_points):
		sum += (ypredict[each_datapt] - train_Y[each_datapt]) ** 2

	loss = sum/(2 * n_data_points)

	return loss

###################
# YOUR CODE BELOW #
###################
def linreg_grad_desc(initial_Theta, train_X, train_Y, alpha=0.05, num_iters=500, print_iters=True):
	'''
	Fits parameters using gradient descent


	Args:
		initial_Theta ((D+1)-by-1 numpy array): The initial value for the parameters we're optimizing over
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (N-by-1 numpy array): The training data target as a vector
		alpha (float): the learning rate/step size, defaults to 0.05
		num_iters (int): number of iterations to run gradient descent for, defaults to 500


	Returns:
		The history of theta's and their associated loss as a list of tuples [ (Theta1,loss1), (Theta2,loss2), ...]
	'''
	cur_Theta = initial_Theta
	step_history = list()

	for k in range(1,num_iters+1):
		# print("k", k)
		cur_loss = loss(cur_Theta, train_X, train_Y)
		step_history.append((cur_Theta, cur_loss))
		if print_iters:
			print("Iteration: {} , Loss: {} , Theta: {}".format(k,cur_loss,cur_Theta))
		#TODO: Add update equation here

		n_var = train_X.shape[0]
		arr_identity = numpy.ones([n_var, 1])

		x = numpy.append(arr_identity, train_X, axis=1)

		xt = numpy.transpose(x)
		xtx = numpy.dot(xt, x)
		xtxtheta = numpy.dot(xtx, cur_Theta)
		xty = numpy.dot(xt, train_Y)

		gradient = (xtxtheta - xty) / train_X.shape[0]

		cur_Theta = cur_Theta - alpha*gradient

	return step_history

def apply_RFF_transform(X,Omega,B):
	'''
	Transforms features into a Fourier basis with given samples

		Given a set of random inner products and translations, transform X into the Fourier basis, Phi(X)
			phi_k(x) = cos(<x,omega_k> + b_k)                           #scalar form
			Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)].T  #vector form
			Phi(X) = [Phi(x_1), Phi(x_2), ..., Phi(x_N)].T              #matrix form


	Args:
		X (N-by-D numpy array): matrix of row-vector features (may also be a single row-vector)
		Omega (D-by-NFF numpy array): matrix of row-vector inner products
		B (NFF length numpy array): vector of translations



	Returns:
		A N-by-NFF numpy array matrix of transformed points, Phi(X)
	'''
	return numpy.sqrt(1.0/Omega.shape[1])*numpy.cos(X.dot(Omega)+B)

##################
# YOUR CODE HERE #
##################
def random_fourier_features(train_X, train_Y, num_fourier_features=100, alpha=0.05, num_iters=500, print_iters=False):
	'''
	Creates a random set of Fourier basis functions and fits a linear model in this space.

		Randomly sample num_fourier_features's non-linear transformations of the form:

			phi_k(x) = cos(<x,omega_k> + b_k)
			Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)]

		where omega_k and b_k are sampled according to (Rahimi and Recht, 20018). 


	Args:
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector
		num_fourier_features (int): the number of random features to generate


	Returns:
		Theta (numpy array of length num_fourier_features+1): the weights for the *transformed* model
		Omega (D-by-num_fourier_features numpy array): the inner product term of the transformation
		B (numpy array of length num_fourier_features): the translation term of the transformation
	'''
	# You will find the following functions useful for sampling:
	# 	numpy.random.multivariate_normal() for normal random variables
	#	numpy.random.random() for Uniform random variables

	# Omega = test_arr 	# TODO: sample inner-products
	# B = 1	# TODO: sample translations
	gamma = 0.5

	# test_arr = numpy.full((train_X.shape[1], num_fourier_features), 1)

	n, d = train_X.shape
	B = numpy.random.uniform(low=0, high=2 * numpy.pi, size=(1, num_fourier_features))
	Omega = numpy.random.multivariate_normal(mean=numpy.zeros(d), cov=2 * 1/d * numpy.eye(d), size=num_fourier_features)  # m x d

	Omega = Omega.T
	# Omega = (2*numpy.pi)**(-train_X.shape[1]/2) * numpy.exp(-numpy.linalg.norm(num_fourier_features)/2)
	# Omega = numpy.sqrt(2*gamma) * numpy.random.normal(size=(train_X.shape[1], num_fourier_features))

	#Omega = ((1/(numpy.sqrt(2*math.pi) ** train_X.shape[1] )) * (numpy.exp(train_X.shape[1])))* numpy.random.normal(size=(train_X.shape[1], num_fourier_features))

	# B = 2 * math.pi * numpy.random.normal(size=num_fourier_features)
	Phi = apply_RFF_transform(train_X,Omega,B)
	# here's an example of using numpy.random.random()
	# to generate a vector of length = (num_fourier_features), between -0.1 and 0.1
	initial_Theta = (numpy.random.random(size=(num_fourier_features+1,1))-0.5)*0.2

	# arr_identity_phi = numpy.ones([Phi.shape[0], 1])
	#
	# Phi = numpy.append(arr_identity_phi, Phi, axis=1)

	# print("initial theta shape: ", initial_Theta.shape)
	step_history = linreg_grad_desc(initial_Theta,Phi,train_Y,alpha=alpha,num_iters=num_iters,print_iters=print_iters)
	return step_history[-1][0], Omega, B

def rff_model_sample(Theta,Omega,B,model_X):
	sampled_X = numpy.linspace(model_X.min(axis=0),model_X.max(axis=0),100)
	Phi = apply_RFF_transform(sampled_X,Omega,B)

	arr_identity_phi = numpy.ones([Phi.shape[0], 1])

	Phi = numpy.append(arr_identity_phi, Phi, axis=1)

	sampled_Y = Phi.dot(Theta)
	return sampled_X, sampled_Y

def vis_rff_model(train_X, train_Y, Theta, Omega, B):
	sample_X, sample_Y = rff_model_sample(Theta,Omega,B, train_X)
	plot_helper(train_X, train_Y, sample_X, sample_Y)

def linear_reg_fun_plot():
	data_X, data_Y = load_data('1D-no-noise-lin.txt')

	tdata_X, tdata_Y = load_data('1D-no-noise-lin.txt')
	arr_identity = numpy.ones([tdata_X.shape[0], 1])

	tdata_X = numpy.append(arr_identity, tdata_X, axis=1)
	# print("xy ", data_X)
	Theta = linreg_closed_form(data_X, data_Y)
	comp = numpy.linalg.lstsq(tdata_X, tdata_Y, rcond=None)


	print("theta: ", Theta)
	print("comp: ", comp)

	loss_var = loss(Theta, data_X, data_Y)
	print("loss: ", loss_var)

	print("testing whether matching with lstsq: ", loss_var * 2 * (tdata_X.shape[0]))

	vis_linreg_model(data_X, data_Y, Theta)


	# 2D data
	data_X, data_Y = load_data('2D-noisy-lin.txt')

	tdata_X, tdata_Y = load_data('2D-noisy-lin.txt')
	arr_identity = numpy.ones([tdata_X.shape[0], 1])

	tdata_X = numpy.append(arr_identity, tdata_X, axis=1)
	# print("xy ", data_X)
	Theta = linreg_closed_form(data_X, data_Y)
	comp = numpy.linalg.lstsq(tdata_X, tdata_Y, rcond=None)


	print("theta: ", Theta)
	print("comp: ", comp)

	loss_var = loss(Theta, data_X, data_Y)
	print("loss: ", loss_var)

	print("testing whether matching with lstsq: ", loss_var * 2 * (tdata_X.shape[0]))

	vis_linreg_model_2d(data_X, data_Y, Theta)

def	linear_reg_fun_with_feat_duplication():

	data_X, data_Y = load_data('2D-noisy-lin.txt')

	# print(data_X)
	dup = data_X[:,[-1]]
	# print(dup)
	# duplicate one of the features
	data_X = numpy.hstack((data_X, dup))

	# print(data_X)


	# Will generate error as it the determinant of (x.T)x is now 0 due to duplication of a feature column
	try:
		Theta = linreg_closed_form(data_X, data_Y)
		print("theta: ", Theta)

		loss_var = loss(Theta, data_X, data_Y)
		print("loss: ", loss_var)

		# vis_linreg_model_2d(data_X, data_Y, Theta)
	except:
		print("numpy.linalg.LinAlgError XT.X generated an singular matrix and hence inverse can't be calculated")


def linear_reg_fun_with_row_duplication():
	data_X, data_Y = load_data('1D-no-noise-lin.txt')

	# # print(data_X)
	dup = data_X[-1,:]
	dupy = data_Y[-1,:]
	# print(dup)
	# duplicate last input data of both x and y
	data_X = numpy.vstack((data_X, dup))
	data_Y = numpy.vstack((data_Y, dupy))

	# print(data_X)

	# Will generate error as it the determinant of (x.T)x is now 0 due to duplication of a feature column
	Theta = linreg_closed_form(data_X, data_Y)
	print("theta: ", Theta)

	loss_var = loss(Theta, data_X, data_Y)
	print("loss: ", loss_var)

	vis_linreg_model(data_X, data_Y, Theta)

def grad_desc_plot_default():

	data_X, data_Y = load_data('1D-no-noise-lin.txt')

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=False)

	print("Theta: ", theta_history[-1][0])
	print("Loss: ", theta_history[-1][1])

	arr_identity = numpy.ones([data_X.shape[0], 1])

	dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(dataX, data_Y, rcond=None)
	print("lstsq: ", comp)
	vis_linreg_model(data_X, data_Y, theta_history[-1][0])


	data_X, data_Y = load_data('2D-noisy-lin.txt')

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=False)

	print("Theta: ", theta_history[-1][0])
	print("Loss: ", theta_history[-1][1])


	arr_identity = numpy.ones([data_X.shape[0], 1])

	dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(dataX, data_Y, rcond=None)
	print("lstsq: ", comp)
	vis_linreg_model_2d(data_X, data_Y, theta_history[-1][0])

def grad_desc_plot_parameters(alpha, num_iterations):

	# 1D DATA
	data_X, data_Y = load_data('1D-no-noise-lin.txt')

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=False, alpha=alpha, num_iters=num_iterations)

	print("Theta: ", theta_history[-1][0])
	print("Loss: ", theta_history[-1][1])

	arr_identity = numpy.ones([data_X.shape[0], 1])

	dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(dataX, data_Y, rcond=None)
	print("lstsq: ", comp)
	vis_linreg_model(data_X, data_Y, theta_history[-1][0])

	# 2D DATA

	data_X, data_Y = load_data('2D-noisy-lin.txt')

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=False, alpha=alpha, num_iters=num_iterations)

	print("Theta: ", theta_history[-1][0])
	print("Loss: ", theta_history[-1][1])

	arr_identity = numpy.ones([data_X.shape[0], 1])

	dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(dataX, data_Y, rcond=None)
	print("lstsq: ", comp)
	vis_linreg_model_2d(data_X, data_Y, theta_history[-1][0])

def grad_desc_plot_feat_duplicate():
	data_X, data_Y = load_data('2D-noisy-lin.txt')

	dup = data_X[:, [-1]]

	data_X = numpy.hstack((data_X, dup))

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=False)

	print("Theta: ", theta_history[-1][0])
	print("Loss: ", theta_history[-1][1])

	arr_identity = numpy.ones([data_X.shape[0], 1])

	dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(dataX, data_Y, rcond=None)
	print("lstsq: ", comp)
	# vis_linreg_model_2d(data_X, data_Y, theta_history[-1][0])

def grad_desc_plot_row_duplicate():
	data_X, data_Y = load_data('2D-noisy-lin.txt')

	# # print(data_X)
	dup = data_X[-1, :]
	dupy = data_Y[-1, :]
	# print(dup)
	# duplicate last input data of both x and y
	data_X = numpy.vstack((data_X, dup))
	data_Y = numpy.vstack((data_Y, dupy))

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=False)

	print("Theta: ", theta_history[-1][0])
	print("Loss: ", theta_history[-1][1])

	# arr_identity = numpy.ones([data_X.shape[0], 1])
	#
	# dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(data_X, data_Y, rcond=None)
	print("lstsq: ", comp)
	# vis_linreg_model_2d(data_X, data_Y, theta_history[-1][0])

def grad_desc_plot_1D():
	data_X, data_Y = load_data('1D-no-noise-lin.txt')

	zeroes = numpy.zeros([data_X.shape[1] + 1, 1])
	theta_history = linreg_grad_desc(zeroes, data_X, data_Y, print_iters=True, num_iters=10, alpha=0.05)

	for each in theta_history:
		print("Theta:{} Loss:{} \n".format(each[0], each[1]))


	arr_identity = numpy.ones([data_X.shape[0], 1])

	dataX = numpy.append(arr_identity, data_X, axis=1)
	comp = numpy.linalg.lstsq(dataX, data_Y, rcond=None)
	print("lstsq: ", comp)
	vis_linreg_model(data_X, data_Y, theta_history[-1][0])

def rff_plot():
	# dataff_X, dataff_Y = load_data('1D-exp-samp.txt')
	# dataff_X, dataff_Y = load_data('1D-exp-uni.txt')
	# dataff_X, dataff_Y = load_data('1D-quad-uni.txt')
	dataff_X, dataff_Y = load_data('1D-quad-uni-noise.txt')

	print("data shape: ", dataff_X.shape)
	arr_identity = numpy.ones([dataff_X.shape[0], 1])

	dataX = numpy.append(arr_identity, dataff_X, axis=1)
	thetaff, omegaff, bff = random_fourier_features(dataff_X, dataff_Y, num_fourier_features=5000, num_iters=1000,
													print_iters=False)
	print("thetaff, omegaff, bff :", thetaff, omegaff, bff)

	print("shape :", thetaff.shape)

	# arr_identity = numpy.ones([dataff_X.shape[0], 1])
	# dataff_X = numpy.append(arr_identity, dataff_X, axis=1)

	# thetaff = (numpy.random.random(size=(100,1))-0.5)*0.2

	print(thetaff.shape, omegaff.shape, bff.shape)

	# thetaff = numpy.delete(thetaff, (-1), axis=0)

	# thetaff = thetaff[1:, :]

	vis_rff_model(dataff_X, dataff_Y, thetaff, omegaff, bff)

if __name__ == '__main__':

	linear_reg_fun_plot()
	linear_reg_fun_with_feat_duplication()
	linear_reg_fun_with_row_duplication()
	grad_desc_plot_feat_duplicate()
	grad_desc_plot_row_duplicate()
	grad_desc_plot_1D()
	grad_desc_plot_default()
	grad_desc_plot_parameters(0.05, 25)
	rff_plot()





